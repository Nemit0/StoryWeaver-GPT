{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairTokenizer:\n",
    "    def __init__(self, data_path:str=None) -> None:\n",
    "        \"\"\"\n",
    "        BytePairTokenizer object\n",
    "        \"\"\"\n",
    "        if data_path:\n",
    "            self.load_model(data_path)\n",
    "            return\n",
    "        \n",
    "        self.special_tokens:Dict[str, int] = {\n",
    "            '<BOT>': 0,  # Beginning of Text\n",
    "            '<EOT>': 1,   # End of Text\n",
    "            '</w>': 2     # end of word\n",
    "        }\n",
    "        self.inv_special_tokens:Dict[int, str] = {i: t for t, i in self.special_tokens.items()}\n",
    "\n",
    "        self.token_map: Dict[str, int] = self.special_tokens.copy()\n",
    "        self.inv_map: Dict[int, str] = self.inv_special_tokens.copy()\n",
    "        self.bpe_codes: Dict[Tuple[str, str], int] = {}\n",
    "    \n",
    "    def train(self, corpus: List[str], num_merges: int, verbose:bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Train the Byte Pair Tokenizer to process sentences.\n",
    "        \"\"\"\n",
    "        # Build the vocabulary: map token sequences to their frequencies\n",
    "        vocab = {}\n",
    "        if verbose:\n",
    "            print(\"Building vocabulary...\")\n",
    "        for sentence in tqdm(corpus):\n",
    "            # Split sentence into words with leading whitespace preserved\n",
    "            words = re.findall(r'\\s*\\S+|\\s+', sentence)\n",
    "            for word in words:\n",
    "                # Skip special tokens\n",
    "                if word in self.special_tokens.keys():\n",
    "                    continue\n",
    "                chars = list(word) + ['</w>']\n",
    "                word_tuple = tuple(chars)\n",
    "                vocab[word_tuple] = vocab.get(word_tuple, 0) + 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Vocabulary built.\\nTraining BPE...\")\n",
    "        token_id = len(self.token_map)  # Starting token ID\n",
    "        symbols = set()\n",
    "        for word_tuple in vocab.keys():\n",
    "            symbols.update(word_tuple)\n",
    "        for symbol in symbols:\n",
    "            if symbol not in self.token_map:\n",
    "                self.token_map[symbol] = token_id\n",
    "                token_id += 1\n",
    "        self.inv_map = {i: t for t, i in self.token_map.items()}\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Token map built.\\nMerging tokens...\")\n",
    "        # Perform BPE merges\n",
    "        for i in tqdm(range(num_merges)):\n",
    "            pairs = self._get_pair_counts(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.bpe_codes[best_pair] = i # Record the BPE merge rule\n",
    "            new_symbol = ''.join(best_pair)\n",
    "            if new_symbol not in self.token_map:\n",
    "                self.token_map[new_symbol] = token_id\n",
    "                token_id += 1\n",
    "                self.inv_map[self.token_map[new_symbol]] = new_symbol\n",
    "    \n",
    "    def _get_pair_counts(self, vocab: Dict[Tuple[str], int]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"\n",
    "        Get counts of symbol pairs in the vocabulary\n",
    "        \"\"\"\n",
    "        pairs = {}\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + freq\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab_single(self, pair: Tuple[str, str], vocab: Dict[Tuple[str], int]) -> Dict[Tuple[str], int]:\n",
    "        \"\"\"\n",
    "        Merge all occurrences of the given pair in the vocabulary\n",
    "        \"\"\"\n",
    "        new_vocab = {}\n",
    "        bigram = ''.join(pair)\n",
    "        for word, freq in vocab.items():\n",
    "            w = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                # Merge the pair if found\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                    w.append(bigram)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    w.append(word[i])\n",
    "                    i += 1\n",
    "            new_vocab[tuple(w)] = freq\n",
    "        return new_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_word(args):\n",
    "        pair, word_freq = args\n",
    "        word, freq = word_freq\n",
    "        bigram = ''.join(pair)\n",
    "        w = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                w.append(bigram)\n",
    "                i += 2\n",
    "            else:\n",
    "                w.append(word[i])\n",
    "                i += 1\n",
    "        return tuple(w), freq\n",
    "    \n",
    "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Dict[Tuple[str], int]) -> Dict[Tuple[str], int]:\n",
    "        \"\"\"\n",
    "        Parallel merge of all occurrences of the given pair in the vocabulary using multiprocessing.\n",
    "        \"\"\"\n",
    "        with Pool() as pool:\n",
    "            results = pool.map(self._process_word, [(pair, word_freq) for word_freq in vocab.items()])\n",
    "\n",
    "        new_vocab = {word: freq for word, freq in results}\n",
    "        return new_vocab\n",
    "    \n",
    "    def _get_pairs(self, word: List[str]) -> set:\n",
    "        \"\"\"\n",
    "        Return a set of symbol pairs in a word\n",
    "        \"\"\"\n",
    "        pairs = set()\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs.add((word[i], word[i + 1]))\n",
    "        return pairs\n",
    "    \n",
    "    def _apply_bpe(self, word: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Apply BPE to a list of symbols (a word)\n",
    "        \"\"\"\n",
    "        word = word.copy()\n",
    "        pairs = self._get_pairs(word)\n",
    "        while True:\n",
    "            if not pairs:\n",
    "                break\n",
    "            # Find the highest priority pair to merge\n",
    "            min_pair = None\n",
    "            min_rank = float('inf')\n",
    "            for pair in pairs:\n",
    "                if pair in self.bpe_codes:\n",
    "                    rank = self.bpe_codes[pair]\n",
    "                    if rank < min_rank:\n",
    "                        min_rank = rank\n",
    "                        min_pair = pair\n",
    "            if min_pair is None:\n",
    "                break\n",
    "            # Merge the best pair\n",
    "            new_symbol = ''.join(min_pair)\n",
    "            i = 0\n",
    "            while i < len(word) - 1:\n",
    "                if word[i] == min_pair[0] and word[i + 1] == min_pair[1]:\n",
    "                    word[i:i + 2] = [new_symbol]\n",
    "                    i = max(i - 1, 0)  # Restart from the previous position after a merge\n",
    "                else:\n",
    "                    i += 1\n",
    "            pairs = self._get_pairs(word)\n",
    "        return word\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into BPE tokens with leading whitespace preserved\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        words = re.findall(r'\\s*\\S+|\\s+', text)\n",
    "        for word in words:\n",
    "            chars = list(word) + ['</w>']\n",
    "            bpe_word = self._apply_bpe(chars)\n",
    "            tokens.extend(bpe_word)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, data: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode text data into a list of token IDs\n",
    "        \"\"\"\n",
    "        str_list = self.split_text(data)\n",
    "        token_list = [self.token_map[tok] for tok in str_list]\n",
    "        return token_list\n",
    "    \n",
    "    def decode(self, data: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into text\n",
    "        \"\"\"\n",
    "        tokens = [self.inv_map[i] for i in data]\n",
    "        text = ''\n",
    "        for token in tokens:\n",
    "            if token != '</w>':\n",
    "                text += token.replace('</w>', '')\n",
    "        return text\n",
    "\n",
    "    def save_model(self, target_path:str) -> None:\n",
    "        \"\"\"\n",
    "        Save the model to a file as json file\n",
    "        the json will look like\n",
    "        {\n",
    "            token_map : {...},\n",
    "            bpe_codes : {...}\n",
    "        }\n",
    "        The special tokens are not necessary for simple encoding/decoding\n",
    "        hence it is omitted from the model\n",
    "        \"\"\"\n",
    "        with open(target_path, 'w', encoding=\"UTF-8\") as f:\n",
    "            json.dump({\n",
    "                'token_map': self.token_map,\n",
    "                'bpe_codes': {json.dumps(list(k)): v for k, v in self.bpe_codes.items()}\n",
    "            }, f,\n",
    "             indent=4,\n",
    "              ensure_ascii=False)\n",
    "    \n",
    "    def load_model(self, model_path:str, encoding=\"UTF-8\") -> None:\n",
    "        \"\"\"\n",
    "        Load the model from a json file\n",
    "        JSON doesn't allow tuple object as key\n",
    "        hence the tuple keys are converted to string before saving\n",
    "        and converted back to tuple when loading\n",
    "        \"\"\"\n",
    "        with open(model_path, 'r') as f:\n",
    "            model = json.load(f)\n",
    "        self.token_map = model['token_map']\n",
    "        self.inv_map = {i: t for t, i in self.token_map.items()}\n",
    "        self.bpe_codes = {tuple(json.loads(k)): v for k, v in model['bpe_codes'].items()}\n",
    "\n",
    "def load_tokenizer(path:str = None) -> BytePairTokenizer:\n",
    "    \"\"\"\n",
    "    Load the BytePairTokenizer model from the model folder\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        model_path:str = os.path.join(os.getcwd(), 'model', 'tokenizer.json')\n",
    "    else:\n",
    "        model_path:str = path\n",
    "    tokenizer = BytePairTokenizer(model_path)\n",
    "    # tokenizer.load_model(model_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1777, 4313, 2964, 4313, 3279, 2804, 3914, 3066, 4889, 2871, 10120, 2896, 3070, 3399, 3182, 3474, 5091, 2765, 2963, 3001, 3580, 2796, 3181, 10557, 3698, 3496, 2854, 3874, 4855, 2837, 2871, 7153, 5263, 2772, 5468, 2893, 7311, 3175, 2764, 3175, 2764, 10580]\n",
      "Decoded: Sean Bean has a hard time leaving his role as Eddard Stark . He vows to get revenge against those that assisted in his execution , starting with George R. R. Martin\n"
     ]
    }
   ],
   "source": [
    "# Test the BytePairTokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "text = 'Sean Bean has a hard time leaving his role as Eddard Stark . He vows to get revenge against those that assisted in his execution , starting with George R. R. Martin'\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor, Tensor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_device(device)\n",
    "    print(f\"Using {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_device(device)\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10948\n"
     ]
    }
   ],
   "source": [
    "vocab_size:int = len(tokenizer.token_map)\n",
    "embedding_dim:int = 1536\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear:\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): 입력 피처의 크기\n",
    "            output_size (int): 출력 피처의 크기\n",
    "        \"\"\"\n",
    "        self.input_size: int = input_size\n",
    "        self.output_size: int = output_size\n",
    "        self.weights: Tensor = torch.rand(input_size, output_size) # 가중치 랜덤 초기화\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        입력에 가중치를 단순 행렬곱하여 출력\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): 입력 텐서 [batch_size, input_size]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 출력 텐서 [batch_size, output_size]\n",
    "        \"\"\"\n",
    "        self.inputs: Tensor = inputs\n",
    "        self.output: Tensor = torch.mm(inputs, self.weights) # 단순 행렬곱\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        손실 함수 그래디언트 이전 층으로 전달 및 가중치 그래디언트 계산\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): 상위 레이어로부터 전달된 그래디언트 [batch_size, output_size]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 하위 레이어로 전달할 그래디언트 [batch_size, input_size]\n",
    "        \"\"\"\n",
    "\n",
    "        grad_input: Tensor = torch.mm(grad_output, self.weights.t()) # 단순 행렬곱\n",
    "        self.grad_weights: Tensor = torch.mm(self.inputs.t(), grad_output)\n",
    "        return grad_input\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, input_dim: int, output_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Custom Embedding 레이어 초기화\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): 임베딩할 인덱스의 개수 (예: 단어 집합의 크기)\n",
    "            output_dim (int): 임베딩 벡터의 차원\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # 임베딩 매트릭스를 학습 가능한 파라미터로 초기화\n",
    "        self.weights: Tensor = torch.randn(input_dim, output_dim) * 0.01\n",
    "        self.grad_weights: Tensor = torch.zeros_like(self.weights)\n",
    "\n",
    "    def forward(self, input_indices: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        순전파 과정\n",
    "\n",
    "        Args:\n",
    "            input_indices (Tensor): 정수 인덱스 텐서 (예: [batch_size, sequence_length])\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 임베딩된 벡터 텐서 (예: [batch_size, sequence_length, output_dim])\n",
    "        \"\"\"\n",
    "        self.input_indices = input_indices\n",
    "        # 인덱스를 사용하여 임베딩 벡터 선택\n",
    "        self.output = self.weights[input_indices]\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        역전파 과정\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): 상위 레이어로부터 전달된 그래디언트 (예: [batch_size, sequence_length, output_dim])\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 하위 레이어로 전달할 그래디언트 (임베딩 레이어의 경우 없음)\n",
    "        \"\"\"\n",
    "        # grad_output의 형태: [batch_size, sequence_length, output_dim]\n",
    "        # 이를 [batch_size * sequence_length, output_dim]로 평탄화\n",
    "        grad_flat = grad_output.view(-1, self.output_dim)\n",
    "        # input_indices를 평탄화하여 [batch_size * sequence_length] 형태로 \n",
    "        input_flat = self.input_indices.view(-1)\n",
    "        \n",
    "        # 그래디언트를 초기화\n",
    "        self.grad_weights.zero_()\n",
    "        # 그래디언트 누적\n",
    "        self.grad_weights.index_add_(0, input_flat, grad_flat)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"CustomEmbedding\"\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, max_seq_len: int, embed_size: int):\n",
    "        \"\"\"\n",
    "        위치 인코딩 초기화\n",
    "\n",
    "        Args:\n",
    "            max_seq_len (int): 최대 시퀀스 길이\n",
    "            embed_size (int): 임베딩 차원\n",
    "        \"\"\"\n",
    "        self.embed_size = embed_size\n",
    "        self.pos_encoding = torch.zeros(max_seq_len, embed_size)\n",
    "\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_encoding = self.pos_encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        순전파 과정\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): 임베딩된 입력 텐서 [seq_length, embed_size]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 위치 인코딩이 추가된 텐서 [seq_length, embed_size]\n",
    "        \"\"\"\n",
    "        seq_length, embed_size = x.shape\n",
    "\n",
    "        # Ensure positional encoding matches input size\n",
    "        pos_encoding = self.pos_encoding[:, :seq_length, :]  # Slice for the current sequence length\n",
    "\n",
    "        return x + pos_encoding.to(x.device)  # Add positional encoding to the input tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding and positional encoding layers\n",
    "embedding = Embedding(vocab_size, embedding_dim)\n",
    "pos_encoding = PositionalEncoding(len(encoded), embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: [1777, 4313, 2964, 4313, 3279, 2804, 3914, 3066, 4889, 2871, 10120, 2896, 3070, 3399, 3182, 3474, 5091, 2765, 2963, 3001, 3580, 2796, 3181, 10557, 3698, 3496, 2854, 3874, 4855, 2837, 2871, 7153, 5263, 2772, 5468, 2893, 7311, 3175, 2764, 3175, 2764, 10580]\n",
      "Embedded: torch.Size([42, 1536])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m embedded \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mforward(tensor(encoded))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedded\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m pos_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mpos_encoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional Encoding Applied: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_encoded\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 125\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    순전파 과정\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m        Tensor: 위치 인코딩이 추가된 텐서 [batch_size, seq_length, embed_size]\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     batch_size, seq_length, embed_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Ensure positional encoding matches input size\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     pos_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:, :seq_length, :]  \u001b[38;5;66;03m# Slice for the current sequence length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Perform forward pass\n",
    "print(f\"Input Tokens: {encoded}\")\n",
    "embedded = embedding.forward(tensor(encoded))\n",
    "print(f\"Embedded: {embedded.shape}\")\n",
    "pos_encoded = pos_encoding.forward(embedded)\n",
    "print(f\"Positional Encoding Applied: {pos_encoded.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
